{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90791,"databundleVersionId":10592855,"sourceType":"competition"},{"sourceId":258552,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":220995,"modelId":242771}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/test.csv\")\nsample_submission = pd.read_csv('/kaggle/input/System-Threat-Forecaster/sample_submission.csv')\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming `X` is your feature matrix and `y` is your target variable\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nimport numpy as np\n\n# Assuming you have X (features) and y (target)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define search space\nparam_dist = {\n    'n_estimators': np.arange(100, 500, 50),\n    'learning_rate': np.linspace(0.01, 0.3, 5),\n    'max_depth': np.arange(3, 10),\n    'subsample': np.linspace(0.6, 1.0, 3),\n    'colsample_bytree': np.linspace(0.6, 1.0, 3),\n}\n\n# Initialize model\nxgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n\n# Randomized Search\nrandom_search = RandomizedSearchCV(\n    xgb_clf, param_distributions=param_dist, \n    n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, verbose=1\n)\n\n# Fit the model\nrandom_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best parameters:\", random_search.best_params_)\n\n# Use the best model\nbest_model = random_search.best_estimator_\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df =df.dropna(subset=['target'])\n#df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Identify numerical and categorical columns\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\ncat_cols = df.select_dtypes(include=['object']).columns\nnum_cols = num_cols.drop('target', errors='ignore')\n\n# Impute numerical columns with mean\nimputer_num = SimpleImputer(strategy='mean')\ndf[num_cols] = imputer_num.fit_transform(df[num_cols])\ntest_data[num_cols] = imputer_num.transform(test_data[num_cols])\n\n# Impute categorical columns with most frequent value\nimputer_cat = SimpleImputer(strategy='most_frequent')\ndf[cat_cols] = imputer_cat.fit_transform(df[cat_cols])\ntest_data[cat_cols] = imputer_cat.transform(test_data[cat_cols])\n\nprint(\"Missing values handled successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['DateAS'] = pd.to_datetime(df['DateAS']).dt.month\ndf['DateOS'] = pd.to_datetime(df['DateOS']).dt.month\ntest_data['DateAS'] = pd.to_datetime(test_data['DateAS']).dt.month\ntest_data['DateOS'] = pd.to_datetime(test_data['DateOS']).dt.month","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cardinality = df.nunique().sort_values(ascending=False)\n\nlow_cardinality = cardinality[cardinality == 1].index.tolist()\ndf = df.drop(columns=low_cardinality)\ntest_data = test_data.drop(columns=low_cardinality)\ndf.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_cols = df.select_dtypes(include=['object']).columns\n\nselected_features = [col for col in categorical_cols if df[col].nunique() <= 10]\nhigh_cardinality_features = [col for col in categorical_cols if df[col].nunique() > 10]\ndf = pd.get_dummies(df, columns=selected_features, drop_first=True)\ntest_data= pd.get_dummies(test_data, columns=selected_features, drop_first=True)\ndf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nfor col in high_cardinality_features:\n    # Apply Label Encoding\n    df[col] = encoder.fit_transform(df[col])\n    test_data[col] = encoder.fit_transform(test_data[col]) \n\n# Display the transformed dataset\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_cols = df.select_dtypes(include=['int64','int32','float64']).columns.drop('target', errors='ignore')\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\ntest_data[num_cols] = scaler.fit_transform(test_data[num_cols]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df= df.drop(columns=list(set(df.columns)-(set(test_data.columns)|{'target'})), axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, SimpleImputer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset (Replace 'your_data.csv' with actual file path)\ndf = pd.read_csv('your_data.csv')\n\n# Separate features (X) and target (y)\nX = df.drop(columns=['target_column'])  # Replace 'target_column' with actual target column name\ny = df['target_column']\n\n# Identify categorical and numeric columns\ncat_cols = X.select_dtypes(include=['object']).columns\nnum_cols = X.select_dtypes(include=['int64', 'float64']).columns\n\n# Impute missing values\ncat_imputer = SimpleImputer(strategy='most_frequent')\nnum_imputer = SimpleImputer(strategy='mean')\nX[cat_cols] = cat_imputer.fit_transform(X[cat_cols])\nX[num_cols] = num_imputer.fit_transform(X[num_cols])\n\n# Label encode categorical features\nlabel_encoders = {}\nfor col in cat_cols:\n    le = LabelEncoder()\n    X[col] = le.fit_transform(X[col])\n    label_encoders[col] = le\n\n# Scale numeric features\nscaler = StandardScaler()\nX[num_cols] = scaler.fit_transform(X[num_cols])\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# --- Model 1: Decision Tree ---\ndt = DecisionTreeClassifier(random_state=42)\nparam_grid_dt = {\n    'max_depth': [20, 30],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2]\n}\n\ngrid_dt = GridSearchCV(dt, param_grid_dt, cv=3, scoring='accuracy')\ngrid_dt.fit(X_train, y_train)\n\n# Best parameters for Decision Tree\nbest_max_depth = grid_dt.best_params_['max_depth']\nbest_min_samples_split = grid_dt.best_params_['min_samples_split']\nbest_min_samples_leaf = grid_dt.best_params_['min_samples_leaf']\n\ndt_best = grid_dt.best_estimator_\ny_pred_dt = dt_best.predict(X_test)\ndt_accuracy = round(accuracy_score(y_test, y_pred_dt), 2)\n\n# --- Model 2: AdaBoost ---\nadaboost = AdaBoostClassifier(random_state=42)\nparam_grid_ab = {\n    'n_estimators': [10, 20, 30],\n    'learning_rate': [5, 10],\n    'algorithm': ['SAMME']\n}\n\ngrid_ab = GridSearchCV(adaboost, param_grid_ab, cv=3, scoring='accuracy')\ngrid_ab.fit(X_train, y_train)\n\n# Best parameters for AdaBoost\nbest_n_estimators = grid_ab.best_params_['n_estimators']\nbest_learning_rate = grid_ab.best_params_['learning_rate']\n\nab_best = grid_ab.best_estimator_\ny_pred_ab = ab_best.predict(X_test)\nab_accuracy = round(accuracy_score(y_test, y_pred_ab), 2)\n\n# Output results\nprint(f\"Best max_depth for Decision Tree: {best_max_depth}\")\nprint(f\"Best min_samples_split for Decision Tree: {best_min_samples_split}\")\nprint(f\"Best min_samples_leaf for Decision Tree: {best_min_samples_leaf}\")\nprint(f\"Decision Tree Accuracy: {dt_accuracy}\")\n\nprint(f\"Best n_estimators for AdaBoost: {best_n_estimators}\")\nprint(f\"Best learning_rate for AdaBoost: {best_learning_rate}\")\nprint(f\"AdaBoost Accuracy: {ab_accuracy}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Automatically identify categorical columns\ncategorical_cols = df.select_dtypes(include=['object']).columns\ntest_categorical_cols = test_data.select_dtypes(include=['object']).columns\n\n# Apply Label Encoding\nencoder = LabelEncoder()\nfor col in categorical_cols:\n    df[col] = encoder.fit_transform(df[col])\nfor col in test_categorical_cols:\n    test_data[col] = encoder.fit_transform(test_data[col])    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(columns=['target'])   # Features\ny = df['target']     # Target column\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the XGBoost classifier\nxgb = XGBClassifier(n_estimators=500, random_state=42)\n\n# Train the model\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred_xgb)\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = test_data[X.columns]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import pandas as pd\n#df = pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/train.csv\")\n#df_copy = df.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#constant_columns = df.nunique()[df.nunique() == 1]\n#print(constant_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import pandas as pd\n#df = pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/train.csv\")\n#test_data = pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/train.csv\")\n#sample_submission = pd.read_csv('/kaggle/input/System-Threat-Forecaster/sample_submission.csv')\n#df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df =df.dropna(subset=['target'])\n#df.info()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predictions = xgb.predict(test_data)  # Generate predictions\n\n# Create submission DataFrame (Ensure sample_submission exists)\nsubmission = sample_submission.copy()\nsubmission['target'] = test_predictions  \n\n# If classification-like labels are needed (Assuming binary case)\n# Ensure model output is properly rounded or thresholded\nsubmission['target'] = submission['target'].apply(lambda x: '1' if x > 0.5 else '0')\n\n# Save to CSV\nsubmission.to_csv('submission.csv', index=False)\n\n# Output file ready for submission\nprint(\"Submission file created.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df =pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/train.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nX_test =pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/test.csv\")\n\n\n\n\ndf = pd.read_csv(\"/kaggle/input/System-Threat-Forecaster/train.csv\")\n\n\ndf_clean = df.dropna()\n\n\n\n\n\nX =df.drop(\"target\", axis =1)\ny =df[\"target\"]\n\n#from sklearn.dummy import DummyClassifier\n#model =DummyClassifier().fit(X,y)\n#y_pred =model.predict(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#submission =pd.DataFrame({\"id\": range(0,X_test.shape[0]),\n\n#\"target\": y_pred})\n\n#submission.to_csv('submission.csv', index =False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}